{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe17dd5-be05-44c8-99ee-b4295d4ee834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Purpose \n",
    "The goal is to populate the start model for reporting\n",
    "1. Populate dimentions tables ( dim_names[id,name,sex], dim_years[id, year], dim_locations[id,county])\n",
    "2. Populate Fact table (fact_babynames[id, #count, id_dim_names, id_dim_years, id_dim_locations])\n",
    "3. Execute a example query, only for test (not do on production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4c3e9d-21cd-4251-bd8e-fae461040930",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get notebook parameter from Azure pipeline\n",
    "dbutils.widgets.text(\"_pipeline_run_id\",\"0478ce36-b895-48a0-8a08-1b10430247ca\")\n",
    "dbutils.widgets.text(\"_processing_date\",\"22-05-2024\")\n",
    "_pipeline_run_id = dbutils.widgets.get(\"_pipeline_run_id\")\n",
    "silver_processing_date = dbutils.widgets.get(\"_processing_date\")\n",
    "print (_pipeline_run_id)\n",
    "print(silver_processing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4873ce5c-5074-413d-b0e0-59767af1fe68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure my account key and account name so Databricks can access the Data Lake\n",
    "accountName = dbutils.secrets.get(\"dataLakeScope\",\"accountName\")\n",
    "accountKey = dbutils.secrets.get(\"dataLakeScope\",\"accountKey\")\n",
    "sparkProperty = f'fs.azure.account.key.{accountName}.dfs.core.windows.net'\n",
    "spark.conf.set(sparkProperty,accountKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78be0d25-78a1-404a-a0a4-6e51b4a6eb54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the location of my files\n",
    "goldDimNames = f'abfss://gold@{accountName}.dfs.core.windows.net/dim_names'\n",
    "goldDimYears = f'abfss://gold@{accountName}.dfs.core.windows.net/dim_years'\n",
    "goldDimLocations = f'abfss://gold@{accountName}.dfs.core.windows.net/dim_locations'\n",
    "goldFactBabiesNames = f'abfss://gold@{accountName}.dfs.core.windows.net/fact_babynames'\n",
    "silverSource = f'abfss://silver@{accountName}.dfs.core.windows.net/nybabynames'\n",
    "\n",
    "\n",
    "# Table name\n",
    "silver_table_name =  \"silver.new_york_baby_names\"\n",
    "gold_dim_names_table_name =  \"gold.reference_dim_names\"\n",
    "gold_dim_years_table_name =  \"gold.reference_dim_years\"\n",
    "gold_dim_locations_table_name =  \"gold.reference_dim_locations\"\n",
    "gold_fact_babynames_table_name =  \"gold.fact_baby_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef8f008-835e-4218-98d8-478d9afb062e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retrieve the data that has been added today. \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "condition = f\"to_date(_processing_date) == to_timestamp('{silver_processing_date}', 'dd-MM-yyyy')\"\n",
    "dataSilver = spark.read.table(silver_table_name).filter(condition)\n",
    "\n",
    "display(dataSilver.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "01e95244-668d-4811-99b3-267754af40d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the schema and table, if required\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f3aec1e9-eed6-4ffb-9a3f-f543006b07f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dim_names table\n",
    "from pyspark.sql.functions import sha2, concat\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import *\n",
    "\n",
    "# Create gold table structure\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE EXTERNAL TABLE IF NOT EXISTS {gold_dim_names_table_name} (\n",
    "              sid bigint GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1),\n",
    "              first_name string,\n",
    "              sex string, \n",
    "              _keyHash string,\n",
    "              _valueHash string,\n",
    "              _pipeline_run_id string,\n",
    "              _processing_date timestamp)\n",
    "          USING delta LOCATION '{goldDimNames}' \n",
    "          \"\"\")\n",
    "\n",
    "# Create the align to gold table structure and add metadata\n",
    "dimNameDF = dataSilver \\\n",
    "    .withColumn(\"_keyHash\", sha2(concat(col(\"first_name\"), col(\"sex\")), 256)) \\\n",
    "    .withColumn(\"_valueHash\", sha2(concat(col(\"_pipeline_run_id\"), col(\"_processing_date\")), 256)) \\\n",
    "    .drop(\"_input_file_modification_date\") \\\n",
    "    .drop(\"_input_filename\") \\\n",
    "    .drop(\"year\") \\\n",
    "    .drop(\"county\") \\\n",
    "    .drop(\"count\")\n",
    "\n",
    "# Preprocess the source table to eliminate multiple matches\n",
    "dimNameDF = dimNameDF.groupBy(\"_keyHash\").agg(\n",
    "    F.first(\"first_name\").alias(\"first_name\"),\n",
    "    F.first(\"sex\").alias(\"sex\"),\n",
    "    F.first(\"_pipeline_run_id\").alias(\"_pipeline_run_id\"),\n",
    "    F.first(\"_processing_date\").alias(\"_processing_date\"),\n",
    "    F.first(\"_valueHash\").alias(\"_valueHash\")\n",
    ")\n",
    "\n",
    "# check if the goldDimNames contain the delta table\n",
    "if DeltaTable.isDeltaTable(spark, goldDimNames):\n",
    "    deltaTable = DeltaTable.forPath(spark, goldDimNames)\n",
    "    deltaTable.alias(\"target\").merge(\n",
    "        source=dimNameDF.alias(\"src\"),\n",
    "        condition=\"src._keyHash = target._keyHash\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"src._valueHash != target._valueHash\",\n",
    "        set={\n",
    "            \"_pipeline_run_id\": \"src._pipeline_run_id\",\n",
    "            \"_processing_date\": \"src._processing_date\",\n",
    "            \"_keyHash\": \"src._keyHash\",\n",
    "            \"_valueHash\": \"src._valueHash\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"first_name\": \"src.first_name\",\n",
    "            \"sex\": \"src.sex\",\n",
    "            \"_pipeline_run_id\": \"src._pipeline_run_id\",\n",
    "            \"_processing_date\": \"src._processing_date\",\n",
    "            \"_keyHash\": \"src._keyHash\",\n",
    "            \"_valueHash\": \"src._valueHash\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "else:\n",
    "    # We do not want to automatically create the table from data frame because we want the identity column\n",
    "    raise Exception(f\"Delta table: {gold_dim_names_table_name} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f85e0dca-eeaa-482e-8b1e-eddece023a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dim_years table\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import *\n",
    "\n",
    "# Create gold table structure\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE EXTERNAL TABLE IF NOT EXISTS {gold_dim_years_table_name} (\n",
    "              sid bigint GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1),\n",
    "              year int,\n",
    "              _keyHash string,\n",
    "              _valueHash string,\n",
    "              _pipeline_run_id string,\n",
    "              _processing_date timestamp)\n",
    "          USING delta LOCATION '{goldDimYears}' \n",
    "          \"\"\")\n",
    "\n",
    "# Create the align to gold table structure and add metadata\n",
    "\n",
    "dimYearDF = dataSilver \\\n",
    "    .withColumn(\"_keyHash\", sha2(concat(col(\"year\")), 256)) \\\n",
    "    .withColumn(\"_valueHash\", sha2(concat(col(\"_pipeline_run_id\"), col(\"_processing_date\")), 256)) \\\n",
    "    .drop(\"_input_file_modification_date\") \\\n",
    "    .drop(\"_input_filename\") \\\n",
    "    .drop(\"first_name\") \\\n",
    "    .drop(\"sex\") \\\n",
    "    .drop(\"county\") \\\n",
    "    .drop(\"count\")\n",
    "\n",
    "# Preprocess the source table to eliminate multiple matches\n",
    "dimYearDF = dimYearDF.groupBy(\"_keyHash\").agg(\n",
    "    F.first(\"year\").alias(\"year\"),\n",
    "    F.first(\"_pipeline_run_id\").alias(\"_pipeline_run_id\"),\n",
    "    F.first(\"_processing_date\").alias(\"_processing_date\"),\n",
    "    F.first(\"_valueHash\").alias(\"_valueHash\")\n",
    ")\n",
    "\n",
    "# check if the goldDimNames contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, goldDimNames)): \n",
    "\n",
    "    DeltaTable.forPath(spark, goldDimYears).alias(\"target\").merge(\n",
    "        source = dimYearDF.alias(\"src\"),\n",
    "        condition = \"src._keyHash = target._keyHash\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition = \"src._valueHash != target._valueHash\",\n",
    "        set = {\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_keyHash\" : \"src._keyHash\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"year\" : \"src.year\",\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_keyHash\" : \"src._keyHash\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "else:\n",
    "    # We do not want to automatically create the table from data frame because we want the identity column\n",
    "    raise Exception(f\"Delta table: {gold_dim_years_table_name} not found!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "89da9f0e-4fad-4b8d-9b6e-35d945638518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dim_location table\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import *\n",
    "\n",
    "# Create gold table structure\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE EXTERNAL TABLE IF NOT EXISTS {gold_dim_locations_table_name} (\n",
    "              sid bigint GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1),\n",
    "              county string,\n",
    "              _keyHash string,\n",
    "              _valueHash string,\n",
    "              _pipeline_run_id string,\n",
    "              _processing_date timestamp)\n",
    "          USING delta LOCATION '{goldDimLocations}' \n",
    "          \"\"\")\n",
    "\n",
    "# Create the align to gold table structure and add metadata\n",
    "\n",
    "dimLocationDF = dataSilver \\\n",
    "    .withColumn(\"_keyHash\", sha2(concat(col(\"county\")), 256)) \\\n",
    "    .withColumn(\"_valueHash\", sha2(concat(col(\"_pipeline_run_id\"), col(\"_processing_date\")), 256)) \\\n",
    "    .drop(\"_input_file_modification_date\") \\\n",
    "    .drop(\"_input_filename\") \\\n",
    "    .drop(\"first_name\") \\\n",
    "    .drop(\"sex\") \\\n",
    "    .drop(\"year\") \\\n",
    "    .drop(\"count\")\n",
    "\n",
    "# Preprocess the source table to eliminate multiple matches\n",
    "dimLocationDF = dimLocationDF.groupBy(\"_keyHash\").agg(\n",
    "    F.first(\"county\").alias(\"county\"),\n",
    "    F.first(\"_pipeline_run_id\").alias(\"_pipeline_run_id\"),\n",
    "    F.first(\"_processing_date\").alias(\"_processing_date\"),\n",
    "    F.first(\"_valueHash\").alias(\"_valueHash\")\n",
    ")\n",
    "\n",
    "# check if the goldDimNames contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, goldDimNames)): \n",
    "\n",
    "    DeltaTable.forPath(spark, goldDimLocations).alias(\"target\").merge(\n",
    "        source = dimLocationDF.alias(\"src\"),\n",
    "        condition = \"src._keyHash = target._keyHash\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition = \"src._valueHash != target._valueHash\",\n",
    "        set = {\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_keyHash\" : \"src._keyHash\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"county\" : \"src.county\",\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_keyHash\" : \"src._keyHash\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "else:\n",
    "    # We do not want to automatically create the table from data frame because we want the identity column\n",
    "    raise Exception(f\"Delta table: {gold_dim_locations_table_name} not found!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "16df7407-b55a-469a-8914-1156e8cfb937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE EXTERNAL TABLE IF NOT EXISTS {gold_fact_babynames_table_name} (\n",
    "              sid bigint GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1),\n",
    "              nameSid bigint,\n",
    "              yearSid bigint,\n",
    "              locationSid string,\n",
    "              count int,\n",
    "              _keyHash string,\n",
    "              _valueHash string,\n",
    "              _pipeline_run_id string,\n",
    "              _processing_date timestamp)\n",
    "          USING delta LOCATION '{goldFactBabiesNames}' \n",
    "          \"\"\")\n",
    "\n",
    "# get only the required columns from dim tables\n",
    "dataDimNames = spark.read.table(gold_dim_names_table_name).select(\"first_name\",\"sex\", col(\"sid\").alias(\"nameSid\"))\n",
    "dataDimYears = spark.read.table(gold_dim_years_table_name).select(\"year\",col(\"sid\").alias(\"yearSid\"))\n",
    "dataDimLocations = spark.read.table(gold_dim_locations_table_name).select(\"county\", col(\"sid\").alias(\"locationSid\"))\n",
    "\n",
    "# Join all the data to find the matching SID\n",
    "mappedData = dataSilver \\\n",
    "    .join(dataDimNames, (dataSilver.first_name == dataDimNames.first_name) & (dataSilver.sex == dataDimNames.sex), how=\"left\") \\\n",
    "    .join(dataDimYears, (dataSilver.year == dataDimYears.year), how=\"left\") \\\n",
    "    .join(dataDimLocations, (dataSilver.county == dataDimLocations.county), how=\"left\") \n",
    "\n",
    "mappedData = mappedData.select(\"nameSid\", \"yearSid\", \"locationSid\", \"count\",\"_pipeline_run_id\",\"_processing_date\")\n",
    "\n",
    "# if any unmatch, the value = -1.\n",
    "mappedData = mappedData \\\n",
    "        .withColumn(\"nameSid\", when(col(\"nameSid\").isNull(), lit(-1)).otherwise(col(\"nameSid\"))) \\\n",
    "        .withColumn(\"yearSid\", when(col(\"yearSid\").isNull(), lit(-1)).otherwise(col(\"yearSid\"))) \\\n",
    "        .withColumn(\"locationSid\", when(col(\"locationSid\").isNull(), lit(-1)).otherwise(col(\"locationSid\")))\n",
    "\n",
    "# All the SIDs are part of the keyHash\n",
    "\n",
    "dataFact = mappedData \\\n",
    "    .withColumn(\"_keyHash\", sha2(concat(col(\"nameSid\"), col(\"yearSid\"), col(\"locationSid\")), 256)) \\\n",
    "    .withColumn(\"_valueHash\", sha2(concat(col(\"count\"),col(\"_pipeline_run_id\"),col(\"_processing_date\")), 256))\n",
    "\n",
    "# Preprocess the source table to eliminate multiple matches\n",
    "dataFact = dataFact.groupBy(\"_keyHash\").agg(\n",
    "    F.first(\"nameSid\").alias(\"nameSid\"),\n",
    "    F.first(\"yearSid\").alias(\"yearSid\"),\n",
    "    F.first(\"locationSid\").alias(\"locationSid\"),\n",
    "    F.first(\"count\").alias(\"count\"),\n",
    "    F.first(\"_pipeline_run_id\").alias(\"_pipeline_run_id\"),\n",
    "    F.first(\"_processing_date\").alias(\"_processing_date\"),\n",
    "    F.first(\"_valueHash\").alias(\"_valueHash\")\n",
    ")\n",
    "\n",
    "# check if the goldFactBabiesNames contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, goldFactBabiesNames)): \n",
    "\n",
    "    DeltaTable.forPath(spark, goldFactBabiesNames).alias(\"target\").merge(\n",
    "        source = dataFact.alias(\"src\"),\n",
    "        condition = \"src._keyHash = target._keyHash\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition = \"src._valueHash != target._valueHash\",\n",
    "        set = {\n",
    "            \"count\" : \"src.count\",\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"nameSid\" : \"src.nameSid\",\n",
    "            \"yearSid\" : \"src.yearSid\",\n",
    "            \"locationSid\" : \"src.locationSid\",\n",
    "            \"count\" : \"src.count\",\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_keyHash\" : \"src._keyHash\",\n",
    "            \"_valueHash\" : \"src._valueHash\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "else:\n",
    "    # We do not want to automatically create the table from data frame because we want the identity column\n",
    "    raise Exception(f\"Delta table: {gold_fact_babynames_table_name} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a87b9a4-6956-4edf-be97-d51840f29843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>first_name</th><th>total_count</th></tr></thead><tbody><tr><td>LIAM</td><td>404</td></tr><tr><td>NOAH</td><td>246</td></tr><tr><td>LUCAS</td><td>136</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "LIAM",
         404
        ],
        [
         "NOAH",
         246
        ],
        [
         "LUCAS",
         136
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 48
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT n.first_name, SUM(f.count) AS total_count\nFROM gold.FactBabiesNames f\nJOIN gold.reference_DimNames n ON f.nameSid = n.sid\nJOIN gold.reference_DimYears y ON f.yearSid = y.sid\nWHERE n.sex = 'M' AND y.year = 2021\nGROUP BY n.first_name\nORDER BY total_count DESC\nLIMIT 3) SELECT `first_name`,SUM(`total_count`) `column_14ee90fb2` FROM q GROUP BY `first_name`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "first_name",
             "id": "column_14ee90fb1"
            },
            "y": [
             {
              "column": "total_count",
              "id": "column_14ee90fb2",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_14ee90fb2": {
             "name": "total_count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "ed1c77c6-119b-46af-a81a-89561932f7f1",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 10,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "first_name",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "first_name",
           "type": "column"
          },
          {
           "alias": "column_14ee90fb2",
           "args": [
            {
             "column": "total_count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- three most popular names in 2021\n",
    "-- Check your result for testing. Do not do this in production!\n",
    "\n",
    "--SELECT n.first_name, SUM(f.count) AS total_count\n",
    "--FROM gold.fact_baby_names f\n",
    "--JOIN gold.reference_dim_names n ON f.nameSid = n.sid\n",
    "--JOIN gold.reference_dim_years y ON f.yearSid = y.sid\n",
    "--WHERE n.sex = 'M' AND y.year = 2021\n",
    "--GROUP BY n.first_name\n",
    "---ORDER BY total_count DESC\n",
    "--LIMIT 3;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Maintenance for Data Table\n",
    "\n",
    "# To optimized the performance of the Delta Table, we need to execute 2 commands:\n",
    "# 1. optimize(): Optimize the number of files used to store the data.\n",
    "# 2. vacuum(): remove the ild version of the data. It reduce the overhead but it limites the version we can go back to.\n",
    "\n",
    "\n",
    "# Databricks recommends frequently running the OPTIMIZE command to compact small files.\n",
    "# This operation does not remove the old files. To remove them, run the VACUUM command (https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum).\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/delta/best-practices#--compact-files\n",
    "\n",
    "# In azure we could do predictive optimization (https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#what-operations-does-predictive-optimization-run), it have prerequisites, like a premium plan and managed tables(https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#prerequisites-for-predictive-optimization)\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, gold_dim_names_table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, gold_dim_years_table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, gold_dim_locations_table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, gold_fact_babynames_table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "ed1c77c6-119b-46af-a81a-89561932f7f1",
       "elementType": "command",
       "guid": "af01938a-a2e7-493b-811a-2ff361b811f2",
       "options": null,
       "position": {
        "height": 10,
        "width": 24,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "21c2a469-b93a-48d0-9402-021ba4f6fa31",
     "origId": 1911730025274843,
     "title": "goldDashbord",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1911730025274842,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silverToGold",
   "widgets": {
    "_pipeline_run_id": {
     "currentValue": "0478ce36-b895-48a0-8a08-1b10430247ca",
     "nuid": "7a71f4e8-fcc7-4580-b680-759cae47022f",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "0478ce36-b895-48a0-8a08-1b10430247ca",
      "label": null,
      "name": "_pipeline_run_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "_processing_date": {
     "currentValue": "23-05-2024",
     "nuid": "f4fc8eec-629d-4452-ac41-9c3f2797ad5a",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "22-05-2024",
      "label": null,
      "name": "_processing_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
