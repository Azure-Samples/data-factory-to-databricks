{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b447d768-2b3f-4649-a6ac-81cc6cb45beb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Purpose\n",
    "1- Read data from bronze  \n",
    "2- Clean data and avoid duplications  \n",
    "3- Adapt some data format or structure if needed (We rename a column)  \n",
    "4- Save the data processed in a delta table, and save the worng data on another data table for a data analyst.   \n",
    "5- Check version and maintain delta table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39623ee2-a9fa-4926-be7f-5bc2ef634586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get notebook parameter from Azure pipeline\n",
    "dbutils.widgets.text(\"_pipeline_run_id\",\"0478ce36-b895-48a0-8a08-1b10430247ca\")\n",
    "dbutils.widgets.text(\"_processing_date\",\"21-05-2024\")\n",
    "_pipeline_run_id = dbutils.widgets.get(\"_pipeline_run_id\")\n",
    "bronze_processing_date = dbutils.widgets.get(\"_processing_date\")\n",
    "print (_pipeline_run_id)\n",
    "print(bronze_processing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c744dc0e-ec9a-44bf-baf1-e3e76002e148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure my account key and account name so Databricks can access the Data Lake\n",
    "accountName = dbutils.secrets.get(\"dataLakeScope\",\"accountName\")\n",
    "accountKey = dbutils.secrets.get(\"dataLakeScope\",\"accountKey\")\n",
    "sparkProperty = f'fs.azure.account.key.{accountName}.dfs.core.windows.net'\n",
    "spark.conf.set(sparkProperty,accountKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a815703-e57b-4825-97a4-9af30089ab1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the location of my files\n",
    "bronzeSource = f'abfss://bronze@{accountName}.dfs.core.windows.net/nybabynames'\n",
    "silverTarget = f'abfss://silver@{accountName}.dfs.core.windows.net/nybabynames'\n",
    "silverErrors = f'abfss://silver@{accountName}.dfs.core.windows.net/nybabynameserrors'\n",
    "\n",
    "bronze_table_name =  \"bronze.new_york_baby_names\"\n",
    "silver_table_name =  \"silver.new_york_baby_names\"\n",
    "silver_errors_table_name =  \"silver.new_york_baby_names_errors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e7804f-3ac5-48ef-b09a-4b7d5086323f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from Data Lake\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# retrieve the data that has been added today. \n",
    "condition = f\"to_date(_processing_date) == to_timestamp('{bronze_processing_date}', 'dd-MM-yyyy')\"\n",
    "gridDataBronze = spark.read.table(bronze_table_name).filter(condition)\n",
    "\n",
    "display(gridDataBronze.printSchema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40878786-329f-407d-b20b-5af5ac7052ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "\n",
    "# Correct data structure and add metadata\n",
    "\n",
    "# 1. Rename count column \n",
    "gridDataBronze = gridDataBronze.withColumnRenamed(\"name_count\", \"count\")\n",
    "\n",
    "display(gridDataBronze.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320a8a77-09a3-4662-8b97-cad8d68601ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "from  pyspark.sql import *\n",
    "\n",
    "#  Data Quality\n",
    "gridCleanDF = gridDataBronze.filter(\"year IS NOT NULL AND first_name IS NOT NULL AND county IS NOT NULL AND sex IS NOT NULL AND count IS NOT NULL AND count > 0\")\n",
    "\n",
    "\n",
    "# Data Duplication \n",
    "# Window Specification: Define a window specification to partition the data by \"year\", \"first_name\", \"county\", and \"sex\", and order each partition by \"_input_file_modification_date\" in descending order.\n",
    "gridDataWindowSpec = Window.partitionBy(\"year\",\"first_name\",\"county\",\"sex\").orderBy(col(\"_input_file_modification_date\").desc(),\"count\")\n",
    "# Row Number: Add a row number to each row within its partition using the row_number function.Filter Duplicates: Filter the DataFrame to keep only the rows where the row number is 1, effectively keeping the latest record within each partition.\n",
    "findLatestDF = gridCleanDF.withColumn(\"row_number\",row_number().over(gridDataWindowSpec)).filter(\"row_number == 1\").drop(\"row_number\")\n",
    "\n",
    "# Wrong data detected\n",
    "gridDataErrorDF = gridDataBronze.subtract(findLatestDF)\n",
    "\n",
    "gridDataDf = findLatestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296cecf0-bec2-4705-9df0-89602caa4a48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wrong data is saved on a delta table, then a data analist could check it\n",
    "from delta.tables import *\n",
    "\n",
    "# check if the silver contain the delta table for wrong data\n",
    "if(DeltaTable.isDeltaTable(spark, silverErrors)): \n",
    "    # If yes, add data with the existing delta table\n",
    "    gridDataErrorDF.write.mode(\"append\").format(\"delta\").save(silverErrors)\n",
    "else:\n",
    "\n",
    "    # If no, save the the data \n",
    "    gridDataErrorDF.write.mode(\"overwrite\").format(\"delta\").save(silverErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa7f5e8-f4ff-4dfa-a67d-e9cbbdbb97fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# check if the silver contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, silverTarget)): \n",
    "\n",
    "    # If yes, merge data with the existing delta table\n",
    "    DeltaTable.forPath(spark, silverTarget).alias(\"target\").merge(\n",
    "        source = gridDataDf.alias(\"src\"),\n",
    "        condition = \"target.year = src.year and target.first_name = src.first_name and target.county = src.county and target.sex = src.sex\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition = \"target._input_file_modification_date < src._input_file_modification_date\",\n",
    "        set = {\n",
    "            \"count\" : \"src.count\",\n",
    "            \"_processing_date\" : \"src._processing_date\",\n",
    "            \"_pipeline_run_id\" : \"src._pipeline_run_id\",\n",
    "            \"_input_filename\" : \"src._input_filename\",\n",
    "            \"_input_file_modification_date\" : \"src._input_file_modification_date\"\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "\n",
    "    # If no, save the data to silver\n",
    "    gridDataDf.write.mode(\"overwrite\").format(\"delta\").save(silverTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d279b098-c8b7-4019-b382-7b403e1aa787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the schema and table, if required\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {silver_table_name} USING delta LOCATION '{silverTarget}'\")\n",
    "\n",
    "# Note: Using spark.sql because we can use f-string to retrieve the silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40a2c8d-5f52-484c-8255-c573ff15907d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is not necessary from a pipeline perspective; it involves checking table information as a learning experience.\n",
    "\n",
    "DESCRIBE EXTENDED silver.new_york_baby_names\n",
    "\n",
    "-- Location: stored in the storage account\n",
    "-- Provider (format): Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560467df-80ed-49d6-82bb-48fec7f08957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is not necessary from a pipeline perspective; it involves showing the transaction log on the delta version as a learning experience.\n",
    "\n",
    "SELECT version, operationMetrics, operationMetrics.numOutputRows, operationMetrics.numTargetRowsInserted, operationMetrics.numTargetRowsUpdated, operationMetrics.numTargetRowsDeleted\n",
    "FROM (DESCRIBE HISTORY silver.new_york_baby_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9898ce3-e620-4ca3-aab1-b698ac712218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Check your result for testing. Do not do this in production!\n",
    "-- SELECT first_name, sum(count) as cnt\n",
    "-- FROM silver.new_york_baby_names\n",
    "-- GROUP BY (first_name)\n",
    "-- ORDER BY cnt DESC\n",
    "-- LIMIT 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67f3a586-3e8f-4940-8f9c-0087288d15df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Maintenance for Data Table\n",
    "\n",
    "# To optimized the performance of the Delta Table, we need to execute 2 commands:\n",
    "# 1. optimize(): Optimize the number of files used to store the data.\n",
    "# 2. vacuum(): remove the ild version of the data. It reduce the overhead but it limites the version we can go back to.\n",
    "\n",
    "# Databricks recommends frequently running the OPTIMIZE command to compact small files.\n",
    "# This operation does not remove the old files. To remove them, run the VACUUM command (https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum).\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/delta/best-practices#--compact-files\n",
    "\n",
    "# In azure we could do predictive optimization (https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#what-operations-does-predictive-optimization-run), it have prerequisites, like a premium plan and managed tables(https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#prerequisites-for-predictive-optimization)\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, silver_table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days\n",
    "\n",
    "if(DeltaTable.isDeltaTable(spark, silver_errors_table_name)): \n",
    "  gridDataDelta = DeltaTable.forName(spark, silver_errors_table_name)\n",
    "  # In this example, we will run and vacuum every 30 days\n",
    "  if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2598784971005238,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronzeToSilver",
   "widgets": {
    "_filename": {
     "currentValue": "nybabynames-20-05-2024.csv",
     "nuid": "892949d2-02a9-4717-b7a2-718666dbae2b",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "nybabynames.csv",
      "label": null,
      "name": "_filename",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "_pipeline_run_id": {
     "currentValue": "123-123",
     "nuid": "071d5985-d32d-4455-a05f-e5dace19ed00",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "123-123",
      "label": null,
      "name": "_pipeline_run_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
