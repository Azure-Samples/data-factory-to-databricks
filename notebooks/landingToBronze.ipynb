{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "The code will do:  \n",
    "1- Read file from landing zone  \n",
    "2- Add control columns like processing date, file, etc.  \n",
    "2- Move the data to a delta table 'as is', always adding information  \n",
    "3- Check and maintain delta table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39623ee2-a9fa-4926-be7f-5bc2ef634586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get notebook parameter from Azure pipeline\n",
    "dbutils.widgets.text(\"_pipeline_run_id\",\"0478ce36-b895-48a0-8a08-1b10430247ca\")\n",
    "dbutils.widgets.text(\"_filename\",\"nybabynames.csv\")\n",
    "dbutils.widgets.text(\"_processing_date\",\"21-05-2024 18:39:52\")\n",
    "_pipeline_run_id = dbutils.widgets.get(\"_pipeline_run_id\")\n",
    "_filename = dbutils.widgets.get(\"_filename\")\n",
    "_processing_date = dbutils.widgets.get(\"_processing_date\")\n",
    "print(_processing_date)\n",
    "print (_pipeline_run_id)\n",
    "print(_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c744dc0e-ec9a-44bf-baf1-e3e76002e148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure my account key and account name so Databricks can access the Data Lake\n",
    "accountName = dbutils.secrets.get(\"dataLakeScope\",\"accountName\")\n",
    "accountKey = dbutils.secrets.get(\"dataLakeScope\",\"accountKey\")\n",
    "sparkProperty = f'fs.azure.account.key.{accountName}.dfs.core.windows.net'\n",
    "spark.conf.set(sparkProperty,accountKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a815703-e57b-4825-97a4-9af30089ab1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the location\n",
    "landingSource = f'abfss://landing@{accountName}.dfs.core.windows.net/{_filename}'\n",
    "bronzeTarget = f'abfss://bronze@{accountName}.dfs.core.windows.net/nybabynames'\n",
    "\n",
    "# Bronze Delta Table\n",
    "table_name = \"bronze.new_york_baby_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e7804f-3ac5-48ef-b09a-4b7d5086323f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read cvs file data from Data Lake\n",
    "gridDataDf = spark.read.option(\"inferSchema\", \"true\").csv(path= landingSource, header=True)\n",
    "\n",
    "display(gridDataDf.printSchema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40878786-329f-407d-b20b-5af5ac7052ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Add audit columnd to the data frame \n",
    "\n",
    "# 1. Adding current time to process this data set\n",
    "# 2. Adding pipepeline run id from ADF\n",
    "# 3. The landig file name. This is useful for debugging prurpose\n",
    "# 4. Modification date. This help identified order of data when the dataset doesn't have a modification date\n",
    "gridDataDf = gridDataDf.withColumn(\"_processing_date\", lit(datetime.strptime(_processing_date, '%d-%m-%Y %H:%M:%S'))) \\\n",
    "                       .withColumn(\"_pipeline_run_id\", lit(_pipeline_run_id)) \\\n",
    "                       .withColumn(\"_input_filename\", input_file_name()) \\\n",
    "                       .withColumn(\"_input_file_modification_date\", col(\"_metadata.file_modification_time\"))\n",
    "\n",
    "display(gridDataDf.printSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa7f5e8-f4ff-4dfa-a67d-e9cbbdbb97fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# check if the bronze contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, bronzeTarget)): \n",
    "\n",
    "    # If yes, add data to the existing delta table\n",
    "    gridDataDf.write.mode(\"append\").format(\"delta\").save(bronzeTarget)\n",
    "else:\n",
    "\n",
    "    # If no, save the file to bronze\n",
    "    gridDataDf.write.mode(\"overwrite\").format(\"delta\").save(bronzeTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d279b098-c8b7-4019-b382-7b403e1aa787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the schema and table, if required\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} USING delta LOCATION '{bronzeTarget}'\")\n",
    "\n",
    "# Note: Using spark.sql because we can use f-string to retrieve the bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40a2c8d-5f52-484c-8255-c573ff15907d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is not necessary from a pipeline perspective; it involves checking table information as a learning experience.\n",
    "\n",
    "DESCRIBE EXTENDED bronze.new_york_baby_names\n",
    "\n",
    "-- Location: stored in the storage account\n",
    "-- Provider (format): Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560467df-80ed-49d6-82bb-48fec7f08957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is not necessary from a pipeline perspective; it involves showing the transaction log on the delta version as a learning experience.\n",
    "\n",
    "SELECT version, operationMetrics, operationMetrics.numOutputRows, operationMetrics.numTargetRowsInserted, operationMetrics.numTargetRowsUpdated, operationMetrics.numTargetRowsDeleted\n",
    "FROM (DESCRIBE HISTORY bronze.new_york_baby_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9898ce3-e620-4ca3-aab1-b698ac712218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Check your result for testing. Do not do this in production!\n",
    "-- SELECT *\n",
    "-- FROM bronze.new_york_baby_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Maintenance for Data Table\n",
    "\n",
    "# To optimized the performance of the Delta Table, we need to execute 2 commands:\n",
    "# 1. optimize(): Optimize the number of files used to store the data.\n",
    "# 2. vacuum(): remove the ild version of the data. It reduce the overhead but it limites the version we can go back to.\n",
    "\n",
    "\n",
    "# Databricks recommends frequently running the OPTIMIZE command to compact small files.\n",
    "# This operation does not remove the old files. To remove them, run the VACUUM command (https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum).\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/delta/best-practices#--compact-files\n",
    "\n",
    "# In azure we could do predictive optimization (https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#what-operations-does-predictive-optimization-run), it have prerequisites, like a premium plan and managed tables(https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization#prerequisites-for-predictive-optimization)\n",
    "\n",
    "gridDataDelta = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# In this example, we will run and vacuum every 30 days\n",
    "if gridDataDelta.history(30).filter(\"operation = 'VACUUM START'\").count() == 0:\n",
    "      gridDataDelta.optimize()\n",
    "      gridDataDelta.vacuum() # default = 7 days"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2598784971005238,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mynotebook",
   "widgets": {
    "_filename": {
     "currentValue": "nybabynames-20-05-2024.csv",
     "nuid": "892949d2-02a9-4717-b7a2-718666dbae2b",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "nybabynames.csv",
      "label": null,
      "name": "_filename",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "_pipeline_run_id": {
     "currentValue": "123-123",
     "nuid": "071d5985-d32d-4455-a05f-e5dace19ed00",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "123-123",
      "label": null,
      "name": "_pipeline_run_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
